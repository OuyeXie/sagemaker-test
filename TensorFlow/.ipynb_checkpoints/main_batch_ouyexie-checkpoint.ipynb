{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the environment¶\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.session import Session\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "region = sagemaker_session.boto_session.region_name\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-181bd7295c81>:7: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "Writing data/train.tfrecords\n",
      "WARNING:tensorflow:From /home/ec2-user/SageMaker/TensorFlow/utils.py:29: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
      "\n",
      "Writing data/validation.tfrecords\n",
      "Writing data/test.tfrecords\n"
     ]
    }
   ],
   "source": [
    "#Download the MNIST dataset¶\n",
    "\n",
    "import utils\n",
    "from tensorflow.contrib.learn.python.learn.datasets import mnist\n",
    "import tensorflow as tf\n",
    "\n",
    "data_sets = mnist.read_data_sets('data', dtype=tf.uint8, reshape=False, validation_size=5000)\n",
    "\n",
    "utils.convert_to(data_sets.train, 'train', 'data')\n",
    "utils.convert_to(data_sets.validation, 'validation', 'data')\n",
    "utils.convert_to(data_sets.test, 'test', 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the data¶\n",
    "\n",
    "inputs = sagemaker_session.upload_data(path='data', key_prefix='data/DEMO-mnist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import os\n",
      "import tensorflow as tf\n",
      "from tensorflow.python.estimator.model_fn import ModeKeys as Modes\n",
      "\n",
      "INPUT_TENSOR_NAME = 'inputs'\n",
      "SIGNATURE_NAME = 'predictions'\n",
      "\n",
      "LEARNING_RATE = 0.001\n",
      "\n",
      "\n",
      "def model_fn(features, labels, mode, params):\n",
      "    # Input Layer\n",
      "    input_layer = tf.reshape(features[INPUT_TENSOR_NAME], [-1, 28, 28, 1])\n",
      "\n",
      "    # Convolutional Layer #1\n",
      "    conv1 = tf.layers.conv2d(\n",
      "        inputs=input_layer,\n",
      "        filters=32,\n",
      "        kernel_size=[5, 5],\n",
      "        padding='same',\n",
      "        activation=tf.nn.relu)\n",
      "\n",
      "    # Pooling Layer #1\n",
      "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
      "\n",
      "    # Convolutional Layer #2 and Pooling Layer #2\n",
      "    conv2 = tf.layers.conv2d(\n",
      "        inputs=pool1,\n",
      "        filters=64,\n",
      "        kernel_size=[5, 5],\n",
      "        padding='same',\n",
      "        activation=tf.nn.relu)\n",
      "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
      "\n",
      "    # Dense Layer\n",
      "    pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])\n",
      "    dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
      "    dropout = tf.layers.dropout(\n",
      "        inputs=dense, rate=0.4, training=(mode == Modes.TRAIN))\n",
      "\n",
      "    # Logits Layer\n",
      "    logits = tf.layers.dense(inputs=dropout, units=10)\n",
      "\n",
      "    # Define operations\n",
      "    if mode in (Modes.PREDICT, Modes.EVAL):\n",
      "        predicted_indices = tf.argmax(input=logits, axis=1)\n",
      "        probabilities = tf.nn.softmax(logits, name='softmax_tensor')\n",
      "\n",
      "    if mode in (Modes.TRAIN, Modes.EVAL):\n",
      "        global_step = tf.train.get_or_create_global_step()\n",
      "        label_indices = tf.cast(labels, tf.int32)\n",
      "        loss = tf.losses.softmax_cross_entropy(\n",
      "            onehot_labels=tf.one_hot(label_indices, depth=10), logits=logits)\n",
      "        tf.summary.scalar('OptimizeLoss', loss)\n",
      "\n",
      "    if mode == Modes.PREDICT:\n",
      "        predictions = {\n",
      "            'classes': predicted_indices,\n",
      "            'probabilities': probabilities\n",
      "        }\n",
      "        export_outputs = {\n",
      "            SIGNATURE_NAME: tf.estimator.export.PredictOutput(predictions)\n",
      "        }\n",
      "        return tf.estimator.EstimatorSpec(\n",
      "            mode, predictions=predictions, export_outputs=export_outputs)\n",
      "\n",
      "    if mode == Modes.TRAIN:\n",
      "        optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
      "        train_op = optimizer.minimize(loss, global_step=global_step)\n",
      "        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
      "\n",
      "    if mode == Modes.EVAL:\n",
      "        eval_metric_ops = {\n",
      "            'accuracy': tf.metrics.accuracy(label_indices, predicted_indices)\n",
      "        }\n",
      "        return tf.estimator.EstimatorSpec(\n",
      "            mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
      "\n",
      "\n",
      "def serving_input_fn(params):\n",
      "    inputs = {INPUT_TENSOR_NAME: tf.placeholder(tf.float32, [None, 784])}\n",
      "    return tf.estimator.export.ServingInputReceiver(inputs, inputs)\n",
      "\n",
      "\n",
      "def read_and_decode(filename_queue):\n",
      "    reader = tf.TFRecordReader()\n",
      "    _, serialized_example = reader.read(filename_queue)\n",
      "\n",
      "    features = tf.parse_single_example(\n",
      "        serialized_example,\n",
      "        features={\n",
      "            'image_raw': tf.FixedLenFeature([], tf.string),\n",
      "            'label': tf.FixedLenFeature([], tf.int64),\n",
      "        })\n",
      "\n",
      "    image = tf.decode_raw(features['image_raw'], tf.uint8)\n",
      "    image.set_shape([784])\n",
      "    image = tf.cast(image, tf.float32) * (1. / 255)\n",
      "    label = tf.cast(features['label'], tf.int32)\n",
      "\n",
      "    return image, label\n",
      "\n",
      "\n",
      "def train_input_fn(training_dir, params):\n",
      "    return _input_fn(training_dir, 'train.tfrecords', batch_size=100)\n",
      "\n",
      "\n",
      "def eval_input_fn(training_dir, params):\n",
      "    return _input_fn(training_dir, 'test.tfrecords', batch_size=100)\n",
      "\n",
      "\n",
      "def _input_fn(training_dir, training_filename, batch_size=100):\n",
      "    test_file = os.path.join(training_dir, training_filename)\n",
      "    filename_queue = tf.train.string_input_producer([test_file])\n",
      "\n",
      "    image, label = read_and_decode(filename_queue)\n",
      "    images, labels = tf.train.batch(\n",
      "        [image, label], batch_size=batch_size,\n",
      "        capacity=1000 + 3 * batch_size)\n",
      "\n",
      "    return {INPUT_TENSOR_NAME: images}, labels\n",
      "\n",
      "def neo_preprocess(payload, content_type):\n",
      "    import logging\n",
      "    import numpy as np\n",
      "    import io\n",
      "\n",
      "    logging.info('Invoking user-defined pre-processing function')\n",
      "\n",
      "    if content_type != 'application/x-image' and content_type != 'application/vnd+python.numpy+binary':\n",
      "        raise RuntimeError('Content type must be application/x-image or application/vnd+python.numpy+binary')\n",
      "    \n",
      "    f = io.BytesIO(payload)\n",
      "    image = np.load(f)*255\n",
      "\n",
      "    return image\n",
      "\n",
      "### NOTE: this function cannot use MXNet\n",
      "def neo_postprocess(result):\n",
      "    import logging\n",
      "    import numpy as np\n",
      "    import json\n",
      "\n",
      "    logging.info('Invoking user-defined post-processing function')\n",
      "    \n",
      "    # Softmax (assumes batch size 1)\n",
      "    result = np.squeeze(result)\n",
      "    result_exp = np.exp(result - np.max(result))\n",
      "    result = result_exp / np.sum(result_exp)\n",
      "\n",
      "    response_body = json.dumps(result.tolist())\n",
      "    content_type = 'application/json'\n",
      "\n",
      "    return response_body, content_type\n"
     ]
    }
   ],
   "source": [
    "# Construct a script for distributed training\n",
    "\n",
    "!cat 'mnist.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tensorflow py2 container will be deprecated soon.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-24 00:47:51 Starting - Starting the training job...\n",
      "2019-12-24 00:47:53 Starting - Launching requested ML instances...\n",
      "2019-12-24 00:48:51 Starting - Preparing the instances for training...."
     ]
    }
   ],
   "source": [
    "# Create an estimator\n",
    "\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "mnist_estimator = TensorFlow(entry_point='mnist.py',\n",
    "                             role=role,\n",
    "                             framework_version='1.12.0',\n",
    "                             training_steps=1000, \n",
    "                             evaluation_steps=100,\n",
    "                             train_instance_count=2,\n",
    "                             train_instance_type='ml.c4.xlarge')\n",
    "\n",
    "mnist_estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mnist_estimator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e9505c3f9278>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# SageMaker's transformer class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtransformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ml.m4.xlarge'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'mnist_estimator' is not defined"
     ]
    }
   ],
   "source": [
    "# SageMaker's transformer class\n",
    "\n",
    "transformer = mnist_estimator.transformer(instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a batch transform job\n",
    "\n",
    "input_bucket_name = 'sagemaker-sample-data-{}'.format(region)\n",
    "input_file_path = 'batch-transform/mnist-1000-samples'\n",
    "\n",
    "transformer.transform('s3://{}/{}'.format(input_bucket_name, input_file_path), content_type='text/csv')\n",
    "\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the results\n",
    "\n",
    "print(transformer.output_path)\n",
    "\n",
    "# Now let's download the first ten results from S3:\n",
    "\n",
    "import json\n",
    "from six.moves.urllib import parse\n",
    "\n",
    "import boto3\n",
    "\n",
    "parsed_url = parse.urlparse(transformer.output_path)\n",
    "bucket_name = parsed_url.netloc\n",
    "prefix = parsed_url.path[1:]\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "predictions = []\n",
    "for i in range(10):\n",
    "    file_key = '{}/data-{}.csv.out'.format(prefix, i)\n",
    "\n",
    "    output_obj = s3.Object(bucket_name, file_key)\n",
    "    output = output_obj.get()[\"Body\"].read().decode('utf-8')\n",
    "\n",
    "    predictions.extend(json.loads(output)['outputs']['classes']['int64Val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demonstration purposes, we're also going to download the corresponding original input data so that we can see how the model did with its predictions.\n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import genfromtxt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (2,10)\n",
    "\n",
    "def show_digit(img, caption='', subplot=None):\n",
    "    if subplot == None:\n",
    "        _,(subplot) = plt.subplots(1,1)\n",
    "    imgr = img.reshape((28,28))\n",
    "    subplot.axis('off')\n",
    "    subplot.imshow(imgr, cmap='gray')\n",
    "    plt.title(caption)\n",
    "\n",
    "tmp_dir = '/tmp/data'\n",
    "if not os.path.exists(tmp_dir):\n",
    "    os.makedirs(tmp_dir)\n",
    "\n",
    "for i in range(10):\n",
    "    input_file_name = 'data-{}.csv'.format(i)\n",
    "    input_file_key = '{}/{}'.format(input_file_path, input_file_name)\n",
    "    \n",
    "    s3.Bucket(input_bucket_name).download_file(input_file_key, os.path.join(tmp_dir, input_file_name))\n",
    "    input_data = genfromtxt(os.path.join(tmp_dir, input_file_name), delimiter=',')\n",
    "\n",
    "    show_digit(input_data)\n",
    "    \n",
    "\n",
    "print(', '.join(predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
